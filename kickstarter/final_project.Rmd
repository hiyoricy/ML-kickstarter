---
title: "[CATCHY TITLE]"
author:
- Hoyt Gong
- Younghu Park
- Hiyori Yoshida
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(car, dplyr, ggplot2, glmnet, tidyverse, jsonlite, lubridate, zoo,tm, SnowballC, RColorBrewer, wordcloud)


```

# Summary 

# Description of the Problem

First, some terms are defined to help explain how Kickstarter works. 

A **creator** is the person or team behind a project idea. 

A **project** is a finite work with a clear goal for which a creator is seeking funding.

The **funding goal** is the amount of money that a creator seeks to raise for their project. 

**Backers** are people who **pledge** money to help creators make their projects a reality. 

Creators who seek funding on Kickstarter want their projects to succeed so they can bring their ideas to life. Funding on Kickstarter is all-or-nothing. This means the creator will not receive the money people have pledged toward their project unless the project reaches its funding goal. Thus, "success" in this case is reaching the funding goal. As demonstrated in Kickstarter's [Creator Handbook](https://www.kickstarter.com/help/handbook?ref=whatarebasics), creators put a lot of effort into designing their project, funding, promotion, and communication to increase their chances of reaching their funding goal. 

The goal of this paper is to use attributes of existing Kickstarter projects in classifiers to predict whether a project will succeed or fail. Determining the atrributes of a project that are most valuable in predicting success can help creators design their projects to maximize their chances of reaching their funding goal. 

# Description of the Data

[Web Robots](https://webrobots.io/kickstarter-datasets/) has been scraping data from Kickstarter projects monthly starting March 2016. The most recent data from November 2019 is used in this project. The response variable that is predicted in this project is `successful`, a categorical variable that is either 1 for "successful" or 0 for "failed". The original data includes "canceled" as a state, which are converted to "failed". This is because it is assumed that canceled projects would likely have failed. The following independent variables are considered in the analysis: 

|Independent Variable|Description|
|-----------------|--------------------------------|
| `backers_count` | The number of backers that have pledged some amount of money to the project. |
| `blurb` | A short summary at the top of project pages that describe/promote the project. |
| `category` | The category of the project. This dataset includes 160 different categories, including "public Art," "software," "narrative film," etc. |
| `slug` -> `description`| Select keywords to describe the project, renamed to description |
| `country` | The country in which the project is based. | 
| `location` | The city and state in wich the project is based. |
| `deadline` | The date and time of the deadline until which the project will raise funds. After the deadline, the project is determined to be "successful" or "failed" depending on whether it reached its funding goal. |
| `launched_at` | The date and time at which the project was made public and started raising funds. |
| `duration_days` | deadline - launched_at. The fundraising duration of the project in days. |
| `goal` | The funding goal of the project. |
| `name` | The title of the project. |
| `usd_pledged` | The amount pledged towards the project in USD. |
| `staff_pick` | True if the Kickstarter staff have chosen to feature the project in their "Projects We Love" program, false otherwise. 

# Data Cleaning and Preparation

```{r}
#to import dataset
kick_data <- read.csv("kick_data.csv", header=TRUE)
```

```{r include = FALSE}
#head(kick_data)
```

```{r}
dim(kick_data)
```

```{r}
colnames(kick_data)
```

First, because our data includes multiple entries for each company whenever the state of their project changes (i.e. from live to failed), we needed to get the rows with the companies most recent states. We did this by finding the max value of `state_changed_at`. Additionally, some rows seemed to contain duplicates of companies, with only the `url` to be different, so we simply utilized the autogenerated index row `X` to drop the duplicates and renamed this as `sub_id` for identification purposes.

```{r}
#filtering out duplicate entries by keeping the most recent item and additionally filtering by the unique column number in X
kick_data.f <- kick_data %>% group_by(id)%>% filter(state_changed_at == max(state_changed_at)) %>% filter(X == max(X)) %>% rename(sub_id=X)
```

Next, we wanted to filter down our columns to the ones we detailed above. We also included the `id` and `sub_id` columns to uniquely identify each row. 

```{r}
#selecting columns to use, temporarily selecting id columns for merging and identification
to_use<- c("id", "sub_id",  "state", "backers_count", "blurb", "slug", "category", "converted_pledged_amount", "country","location", "deadline", "launched_at", "goal", "name","usd_pledged","staff_pick")
kick_data.f <- kick_data.f %>% select(to_use)
```


Next, we converted the `launched_at` and `deadline` columns to date formats.

```{r}
#converting launched_at and deadline to datetime formats
dts <- kick_data.f$launched_at
mydates = dts
class(mydates) = c('POSIXt','POSIXct')
kick_data.f$launched_at <- as.Date(mydates)

dts <- kick_data.f$deadline
mydates = dts
class(mydates) = c('POSIXt','POSIXct')
kick_data.f$deadline <-  as.Date(mydates)
```

Then, we proceeded to create our `duration_days` column to indicate the timespan of the project.

```{r}
#calcuation duration of project in days
kick_data.f$duration_days <- as.numeric(difftime(kick_data.f$deadline ,kick_data.f$launched_at , units = c("days")))
```


Next, we needed to handle to json data within the `category` and `location` columns. For the `category` column, we were able to successfully transform the data into a json and extract the `category` and `subcategory` columns. However, the location column had some erroneous formatting, and we thus manually extracted the name with string manipulation searching for the `displayable_name` tag. In order to cut down the number of unique locations, we also only preserved the country of origin. For projects in the United States, we seperated them by US Region.

```{r}
#parsing our json data to extract categories 
ParseJSONColumn <- function(x)  {
     str_c("[ ", str_c(x, collapse = ",", sep=" "), " ]") %>% 
      fromJSON(flatten = T) %>% 
     as.tibble()
} 

JSONcolumn_data <- kick_data.f  %>% 
    select(id, category)  %>% 
    map_dfc(.f = ParseJSONColumn)  %>% select(value, name, slug) %>% rename(id = value, category = name, subcategory = slug)


kick_data.f <- kick_data.f %>% select(-category) %>% rename(description=slug)

kick_data.f <-  merge(kick_data.f, JSONcolumn_data ,by="id")

#extracting locations through string splits
parsed_location <- kick_data.f %>% separate(location, c("foo", "bar"), '\"displayable_name\":\"') %>% select(foo,bar) %>% separate(bar, c("location", "foo2"), '\",\"') %>%  separate(location, c("foo", "location"), ', ')  %>% select(location)

kick_data.f$location <- parsed_location$location

northeast =c('CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA')
midwest = c('IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD')
west =c('AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AL', 'CA', 'HI', 'OR', 'WA')
south = c('MD', 'DE', 'VA', 'WV','KY','TN','NC', "SC", "FL", "GA", "AL", "MS", "LA", "AK", "TK", "OK")
kick_data.f$location <- ifelse(kick_data.f$location %in% northeast, "US-Northeast", ifelse(kick_data.f$location %in% midwest, "US-Midwest", ifelse(kick_data.f$location %in% west, "US-West", ifelse(kick_data.f$location %in% south, "US-South", kick_data.f$location))))

```


Next, we converted various columns to our desired variable type. We also further cleaned the `description` column to get rid of the hyphens between each words.
```{r}
#converting types of columns
kick_data.f$location <- as.factor(kick_data.f$location)
kick_data.f$blurb <- as.character(kick_data.f$blurb)
kick_data.f$description <- as.character(kick_data.f$description)
kick_data.f$name <- as.character(kick_data.f$name)
kick_data.f$category <- as.factor(kick_data.f$category)

#replacing dashes with spaces in descriptions column and removing _truncated_ tag in blurb
kick_data.f$description <- gsub("-", " ",kick_data.f$description)
```


Finally, we created our indicator column to predict on. First, we filtered out the `live` projects as those were still ongoing and thus were not properly concluded. For the remaining projects, we classified all projects with the `state`, `successful` as 1, while `failed` and `canceled` projects would be 0.

```{r}
#filtering out the live projects, but storing them in a seperate place
kick_data.f.live <- kick_data.f %>% filter(state=="live")
kick_data.f <- kick_data.f %>% filter(state != "live")
```


```{r}
#creating predictor column
kick_data.f$successful<- as.factor(as.numeric(ifelse(kick_data.f$state=="successful", 1, 0)))
```

As only last sanity check, we took only the columns we wanted to move forward with.
```{r}
#double checking that we all desired columns and removing ids
to_use<- c("state", "successful", "backers_count", "blurb", "category", "description", "converted_pledged_amount", "country","location", "deadline", "launched_at", "duration_days", "goal", "name","usd_pledged","staff_pick")

kick_data.f <- kick_data.f %>% select(to_use)
```

```{r}
#remove converted_pledged_amount because it is the same as usd_pledged, just converted differently 
kick_data.f <- subset(kick_data.f, select = -converted_pledged_amount)
```

```{r, include=FALSE}
#check which variables have NAs 
sapply(kick_data.f, function(x) any(is.na(x))) # any(is.na(var)) is very useful, it returns T/F for each var
apply(kick_data.f, 2, function(x) any(is.na(x))) == TRUE  # apply function by columns: column-wise missing 
apply(kick_data.f, 1, function(x) any(is.na(x)))  # apply function by rows: row-wise missing
```

The only columns with NAs appear to be `location` and `blurb.` There is one NA in `blurb` and 117 NAs in `location.` Since there is no reasonable way to interpolate the names of cities in which projects are based or the blurb of the project, rows that have NA values in `location` or `blurb` are removed from the dataset.   

```{r include =FALSE}
sum(is.na(kick_data.f$location))
sum(is.na(kick_data.f$blurb))
```

```{r include=FALSE}
kick_data.f <- na.omit(kick_data.f)
dim(kick_data.f)
str(kick_data.f)
```

The dataset includes 85405 observations and 15 variables. For computational reasons, a quarter of the dataset is randomly sampled and used for the models instead of the entire dataset. 

```{r}
set.seed(10)
#First randomly sample a quarter of the data for computational purposes
sample.size <- round(dim(kick_data.f) * 0.25)
index.sample <- sample(nrow(kick_data.f), sample.size)
data <- kick_data.f[index.sample, ] #dim(data) = 21351        15
```

The final dataset used for the models includes 21351 observations and 15 variables. 

The final dataset is split into training and testing sets, 70% for training and 30% for testing. 

```{r}
#split the data into training and testing sets 
set.seed(10)

# use the 70/30 rule: set the training dataset size to be 70% of data size and testing to be 30%
train.size <- round(dim(data) * 0.7)
index.t <- sample(nrow(data), train.size)
train <- data[index.t, ] #dim(train) = 14946    15
test <- data[-index.t, ] #dim(test) = 6405    15
```

# Exploratory Data Analysis

As shown below, 22 countries are represented in the dataset. 71% of the projects are based in the US. 
```{r}
#Bar chart of Countries 
counts <- table(data$country)
barplot(counts, main="Country Distribution",
   xlab="Country", las=2, col=brewer.pal(22, name="YlGn"))
```

```{r include=FALSE}
15163/sum(counts)
```

Among the 160 categories included in the dataset, "Web" is the most popular, with 429 projects. The top 10 categories, grouped by `successful`, are shown below. 
```{r}
#category EDA 
cat_counts <- table(data$category)
names(cat_counts)[which.max(cat_counts)]
max(cat_counts)

names(cat_counts)[which.min(cat_counts)]
min(cat_counts)

cat_counts.df <- data.frame(cat_counts)

cat.top.10 <- cat_counts.df %>% top_n(10)

cat.top.10 <- data %>% select(successful, category) %>% group_by(category) %>% count() %>% arrange(desc(n)) %>% head(10) %>% ungroup

cat.top.10.grouped <- data %>% filter(category %in% cat.top.10$category) %>% group_by(category, successful) %>% count() %>% ungroup

cat.add <- data.frame(c("Accessories", "Comic Books", "Product Design", "Tabletop Games"),
                      c("0", "0", "0", "0"),
                      c("0", "0", "0", "0"))
names(cat.add) <- c("category", "successful", "n")

cat.top.10.grouped <- rbind(cat.top.10.grouped, cat.add)

ggplot(cat.top.10.grouped, aes(x=category, y=n, group=successful, fill=successful)) + geom_bar(stat="identity", width=0.9) + labs(title="Top 10 Categories", x="Category", y = "Frequency") + scale_fill_brewer(palette="YlGn") + theme_classic() + theme(axis.text.x=element_text(angle=90, hjust=1), axis.text.y=element_blank())
```

The most popular successful category appears to be Hip-Hop, while the least 

The distribution of `backers_count` is highly skewed right. The mean is 139 backers, with a large standard deviation of 980 backers. The median is 26 backers, with a large  interquartile range of 86-3=83 backers. Since the minimum number of backers is 0 while the maximum is 91585, `backers_count` has a wide range. Because of this high range, the violin plot below only shows `backers_count` under 500 people. 

```{r}
data.t1 <- data %>% filter(backers_count < 500)

data %>% filter(backers_count > 500) %>% count()
```


```{r}
#violin plot of log backers count
ggplot(data, aes(x=successful, y=log(backers_count), group=successful, fill=successful)) + geom_violin(trim=FALSE)+ geom_boxplot(width=0.1, fill="white") + labs(title="Successful versus Log of Backers Count",x="Successful", y = "Log(Backers Count)") + scale_fill_brewer(palette="Blues") + theme_classic()

summary(data$backers_count)
sd(data$backers_count)
```

Like `backers_count`, the distribution of `goal` is skewed right. The mean fundraising goal is about \$42900 with a large standard deviation of \$978141. The median is about $5000, with a large IQR of 15000-1500=\$13500. The large range of \$100000000 also shows that `goal` differs greatly among projects. 

```{r}
#violin plot of goal
ggplot(data, aes(x=successful, y=log(goal), group=successful, fill=successful)) + geom_violin(trim=FALSE)+ geom_boxplot(width=0.1, fill="white") + labs(title="Successful versus Log of Goal" ,x="Successful", y = "Log(Goal)") + scale_fill_brewer(palette="Blues") + theme_classic()


summary(data$goal)
sd(data$goal)
```

The histogram of `usd_pledged`, the amount of money pledged to a project in USD, also looks very similar to `backers_count` and `goal`. The mean is \$12975, with a large standard deviation of \$96928. The median is $1500 with a large IQR of 6381-97=\$6284. The range is also large, \$6225355.  

```{r}
#histogram of usd_pledged
ggplot(data, aes(x=successful, y=log(usd_pledged), group=successful, fill=successful)) + geom_violin(trim=FALSE)+ geom_boxplot(width=0.1, fill="white") + labs(title="Successful versus Log of USD Pledged" ,x="Successful", y = "Log(USD Pledged)") + scale_fill_brewer(palette="Blues") + theme_classic()


summary(data$usd_pledged)
sd(data$usd_pledged)
```

As the histogram below shows, the distribution of deadlines for projects is skewed left, which is expected since the projects are those that exist in November 2019. The majority of projects have a deadline in the range 2014-2019. Late 2014 to early 2015 appears to have the most deadlines for projects. 

```{r}
#histogram of deadlines 
histogram <- hist(data$deadline, breaks = 40, xlab = "Deadline Date", ylab = "Frequency", 
               main = "Histogram of Deadline",col = brewer.pal(30, name="YlGn"))
```

The histogram of `duration_days` shows two modes, one at around 30 days and the other at around 60 days. This suggests that most projects are live for about a month, while others are live for about 2 months. The mean is about 32.7 days, with a standard deviation of 12 days. 

```{r}
#histogram of duration_days
histogram <- hist(data$duration_days, breaks = 40, xlab = "Duration in Days", ylab = "Frequency", 
               main = "Histogram of Duration of Project in Days",col = brewer.pal(30, name="YlGn"))
summary(data$duration_days)
sd(data$duration_days)
```

The bar chart of `staff_pick` shows that most projects are not chosen to be featured by the staff as part of the "Projects We Love" program. This suggests that the projects that are chosen are particularly featureable.

```{r}
#Bar chart of staff_pick 
counts <- table(data$staff_pick)
barplot(counts, main="Staff Pick",
   xlab="Staff Pick", las=2, col=brewer.pal(2, name="YlGn"))
```

# Model Overview


For the first model, an Elastic Net is used to limit the size of the coefficient vector and impose sparcity among the coefficients. To locate a set of important features, a logistic regression using `glm()` is run with the variables obtained from Elastic Net on the training dataset.

In all of the following models, `location` is excluded because `country` is a more general variable that describes the location in which a project is based and including both would lead to high colinearity. 

# Models

## Logistic Regression

### Elastic Net 


```{r include = FALSE}
# Prepare the design matrix and response for LASSO
X <- model.matrix(successful ~ data$backers_count + data$category
                  + data$country + data$deadline + data$launched_at
                  + data$duration_days + data$goal + data$usd_pledged + data$staff_pick, data)[, -1]
Y <- data$successful
dim(X)
length(Y)
```

```{r}
set.seed(10) # to have same sets of K folds
fit1.cv <- cv.glmnet(X, Y, alpha=.9, family="binomial", nfolds = 10, type.measure = "deviance")  
plot(fit1.cv)
```

fit1.cv\$lambda.1se is chosen because both fit1.cv\$lambda.min and fit1.cv\$lambda.1se return the same number of variables.

```{r include=FALSE}
coef.1se <- coef(fit1.cv, s="lambda.1se")  
coef.1se <- coef.1se[which(coef.1se !=0),] #pull out coefficients that aren't 0 
coef.1se   
```

Replacing the variables for which factor levels (`categoryPunk`, `countryMX`) are outputted by the above Elastic Net with the corresponding categorical variable, the following eight variables are outputted by Elastic Net: `category`, `backers_count`, `country`, `deadline`, `duration_days`, `staff_pick`, `goal`, and `usd_pledged`. 

### Logistic Regression

```{r include = FALSE}
fit.logit.1 <- glm(successful ~ category + country + duration_days + staff_pick + backers_count
                   + deadline + goal + usd_pledged, family=binomial, data=train)
summary(fit.logit.1)
```

```{r include=FALSE}
Anova(fit.logit.1)
```

Since all variables are significant at the 0.01 level, the final logistic model without the text variables include the following eight variables: `category`, `backers_count`, `country`, `deadline`, `duration_days`, `staff_pick`, `goal`, and `usd_pledged`.

### Evaluate the Logistic Regression Model 

To evaluate the logistic regression model, `fit.logit.1`, the 1/2 thresholding rule is used for predicting `successful` on the testing data.

```{r include=FALSE}
#fit.logit.2
#Predict successful on the testing data.
fit.logit.test <- predict(fit.logit.1, test, type="response")
fit.logit.pred <- ifelse(fit.logit.test >= 1/2, "1", "0")
mce.logit <- mean(fit.logit.pred != test$successful)
mce.logit
```

The testing missclassification error of the logistic regression model is about 0.087. Not bad! 

## Random Forest

```{r, eval=F}
data.text <- data %>% select(successful, backers_count, category, country, deadline, launched_at, duration_days, goal, usd_pledged, staff_pick)
# Combine the original data with the text matrix
data1.temp <- data.frame(data.text, as.matrix(dtm.10))   
#data1.temp$successful <- data$successful
data2 <- data1.temp %>% select(successful, everything())
```

```{r}
#splitting our data into train test splits
set.seed(1)
n=nrow(data2)
test.index <- sample(n, 0.3*n)
data.text.test <- data2[test.index, ] # only keep rating and the texts
data.text.train <- data2[-test.index, ]
```

```{r}
set.seed(1)
library(ranger)

rf.error.p <- 1:14
for (p in 1:14) 
{
  fit.rf <-ranger(successful~., data.text.train, mtry=p, num.trees=250, splitrule = "gini",importance = "impurity", oob.error = TRUE)
  rf.error.p[p] <- fit.rf$prediction.error  # collecting oob mse based on 250 trees
}
rf.error.p # oob mse returned

plot(1:14, rf.error.p, pch=16,
     xlab="mtry",
     ylab="OOB mse of mtry",
     main = "Testing errors as func of mtry with Success as response")
lines(1:14, rf.error.p)
```

```{r}
RF.all <- ranger(successful~., data.text.train, mtry=4, # Although recommended mtry is p=n/3=15/3=5; we tune to get mtry=4
                 num.trees = 250, splitrule = "gini",importance = "impurity")
RF.pred <-predict(RF.all, data.text.test, type = "response")
RF.test.error <- mean(data.text.test$successful!=RF.pred$predictions)

RF.test.error
RF.all$prediction.error
```



## Text Mining 

As an alternative model, we wanted to explore whether or not we could utilize text data contained within the `blurb` column to predict whether or not a company will be successful.

First, we isolated the text data:
```{r}
data1.text <- data$blurb
print(data1.text[1:5]) # view a few documents
```

Overall, it appears as if the blurbs are roughly the same length. This is most likely due to the fact that there is a restriction on length for these blurbs on Kickstarter.

### Corpus: a collection of text

Here, we move onto creating our corpus.

```{r}
mycorpus1 <- VCorpus( VectorSource(data1.text))
mycorpus1
```

### Data cleaning using `tm_map()`

Before we transform our text into a word frequency matrix, we needed to standardized the text. Similar to our model in class, we used  `removeNumbers()`, `removePunctuation()`, `removeWords()`, `stemDocument()`, and `stripWhitespace()` to go about cleaning our data.

```{r}
mycorpus_clean <- tm_map(mycorpus1, content_transformer(tolower))
mycorpus_clean <- tm_map(mycorpus_clean, removeWords, stopwords("english"))
mycorpus_clean <- tm_map(mycorpus_clean, removePunctuation)
mycorpus_clean <- tm_map(mycorpus_clean, removeNumbers)
mycorpus_clean <- tm_map(mycorpus_clean, stemDocument, lazy = TRUE)
```

### Word frequency matrix

Now we transform each review into a word frequency matrix using the function `DocumentTermMatrix()`.

```{r}
dtm1 <- DocumentTermMatrix( mycorpus_clean )   ## library = collection of words for all documents
```


Intially, there are a total of 26541 words that could be utilized as predictors. This is clearly, too many, so we decided to cut down the bag to only include the words appearing at least 1% (or the frequency of your choice) of the time to reduce the dimension of the features extracted to be analyzed. 

```{r}
threshold <- .01*length(mycorpus_clean)   # 1% of the total documents 
words.10 <- findFreqTerms(dtm1, lowfreq=threshold)  # words appearing at least among 1% of the documents
```

```{r}
dtm.10<- DocumentTermMatrix(mycorpus_clean, control = list(dictionary = words.10))  
dim(as.matrix(dtm.10))
colnames(dtm.10)[1:25]
```

In the end, we ended up with 167 possible words to utilize as predictors.

### Data preparation
Next, we used lasso to see what words are actually meaningful predictors for our data.
    
                  
```{r, eval=F}
data.text <- data %>% select(successful, backers_count, category, country, deadline, launched_at, duration_days, goal, usd_pledged, staff_pick)
# Combine the original data with the text matrix
data1.temp <- data.frame(data.text, as.matrix(dtm.10))   
#data1.temp$successful <- data$successful
data2 <- data1.temp %>% select(successful, everything())
```

```{r}
#splitting our data into train test splits
set.seed(1)
n=nrow(data2)
test.index <- sample(n, 0.3*n)
data.text.test <- data2[test.index, ] # only keep rating and the texts
data.text.train <- data2[-test.index, ]
```


###LASSO

```{r}
y.n <- data.text.train$successful
X.n <- model.matrix(successful~., data.text.train)[,-1] # we can use as.matrix directly here
set.seed(1)
#### Be careful to run the following LASSO.
result.lasso.text <- cv.glmnet(X.n, y.n, alpha=0.99, family="binomial")  # 10 minutes in my MAC
# save(result.lasso, file="/Users/lzhao/Dropbox/STAT471/Data/TextMining.RData")
```

```{r}
plot(result.lasso.text)
beta.lasso <- coef(result.lasso.text, s="lambda.1se")   # output lasso estimates
beta <- beta.lasso[which(beta.lasso !=0),] # non zero beta's
beta <- as.matrix(beta)
beta <- rownames(beta)
non_word_beta <- beta[str_detect(beta, "backers_count|category|country|deadline|launched_at|duration_days|goal|usd_pledged|staff_pick")]

word_beta <- beta[!str_detect(beta, "backers_count|category|country|deadline|launched_at|duration_days|goal|usd_pledged|staff_pick")]
non_word_beta
```

Taking a look at our lasso plot, we observe that lasso actually did not cut down any of the words. This suggests that there are no absolutely significant words that are included the description that can lead to success versus failure. Nevertheless, we decided to run a logistic regression on the word frequencies.


```{r}
# Input the words from LASSO into glm
glm.input <- as.formula(paste("successful~category+country+duration_days+goal+usd_pledged+staff_pick+", paste(word_beta[-1],collapse = "+"))) # prepare the formulae
result.glm <- glm(glm.input, family=binomial, data.text.train ) # instantly 

# Testing errors

data.text.test <- data.text.test %>% filter(category != "Taxidermy") 
predict.glm <- predict(result.glm, data.text.test, type = "response")
class.glm <- rep("0", nrow(data.text.test))
class.glm[predict.glm > .5] <- "1"

testerror.glm <- mean(data.text.test$successful != class.glm)
testerror.glm
pROC::roc(data.text.test$successful, predict.glm, plot=T) 
```

Using just the word frequency data, we got a much higher testing error of 0.36455. Using ROC, we saw an area under the curve of 0.6801. This higher error and lower accuracy shows that text data may not be the best in terms of helping us predict whether or not the product will be successful. This shows the overall subjectivity of the ratings.

# Comparison of Methods 

```{r}
library(pROC)
fit1.roc <-roc(test$successful, as.numeric(fit.logit.2.pred)) #Hiroyi
fit2.roc <-roc(data.text.test$successful, predict.glm) #Younghu
fit3.roc <-roc(data.text.test$successful, as.numeric(RF.pred$predictions)) #Hoyt

```


```{r}
plot(1-fit1.roc$specificities,
     fit1.roc$sensitivities, col="red", lwd=3, type="l",
     xlab="False Positive",
     ylab="Sensitivity")
lines(1-fit2.roc$specificities, fit2.roc$sensitivities, col="blue", lwd=3)
lines(1-fit3.roc$specificities, fit3.roc$sensitivities, col="green", lwd=3)
legend("bottomright",
       c(paste0("fit1 AUC=",round(fit1.roc$auc,2)),
         paste0("fit.final AUC=",round(fit2.roc$auc, 2)),
         paste0("fit3 AUC=",round(fit3.roc$auc, 2))),
       col=c("red", "blue", "green"),
       lty=1)

```

The MCEs of the models are as follows:
| Model | MCE |
|-----|-----|
|Logistic Regression without Text| 0.087 |
| Random Forest | 0.080 |
| Logistic Regression with Text | 0.100 |

According to MCE, the most accurate model is the Random Forest model because it has the lowest MCE. 

# Final Model 

# Validity of Results and Future Improvement

# Conclusion

# Appendix

# Works Cited

