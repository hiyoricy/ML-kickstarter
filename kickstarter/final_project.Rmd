---
title: "[CATCHY TITLE]"
author:
- Hoyt Gong
- Younghu Park
- Hiyori Yoshida
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr, ggplot2, glmnet, tidyverse, jsonlite, lubridate, zoo)


```

# Summary 

# Description of the Problem

First, some terms are defined to help explain how Kickstarter works. 

A **creator** is the person or team behind a project idea. 

A **project** is a finite work with a clear goal for which a creator is seeking funding.

The **funding goal** is the amount of money that a creator seeks to raise for their project. 

**Backers** are people who **pledge** money to help creators make their projects a reality. 

Creators who seek funding on Kickstarter want their projects to succeed so they can bring their ideas to life. Funding on Kickstarter is all-or-nothing. This means the creator will not receive the money people have pledged toward their project unless the project reaches its funding goal. Thus, "success" in this case is reaching the funding goal. As demonstrated in Kickstarter's [Creator Handbook](https://www.kickstarter.com/help/handbook?ref=whatarebasics), creators put a lot of effort into designing their project, funding, promotion, and communication to increase their chances of reaching their funding goal. 

The goal of this paper is to use attributes of existing Kickstarter projects in classifiers to predict whether a project will succeed or fail. Determining the atrributes of a project that are most valuable in predicting success can help creators design their projects to maximize their chances of reaching their funding goal. 

# Description of the Data

[Web Robots](https://webrobots.io/kickstarter-datasets/) has been scraping data from Kickstarter projects monthly starting March 2016. The most recent data from November 2019 is used in this project. The response variable that is predicted in this project is `successful`, a categorical variable that is either 1 for "successful" or 0 for "failed". The original data includes "canceled" as a state, which are converted to "failed". This is because it is assumed that canceled projects would likely have failed. The following independent variables are considered in the analysis: 

|Independent Variable|Description|
|-----------------|--------------------------------|
| `backers_count` | The number of backers that have pledged some amount of money to the project. |
| `blurb` | A short summary at the top of project pages that describe/promote the project. |
| `category` | The category of the project. This dataset includes 160 different categories, including "public Art," "software," "narrative film," etc. |
| `slug` -> `description`| Select keywords to describe the project, renamed to description |
| `country` | The country in which the project is based. | 
| `location` | The city and state in wich the project is based. |
| `deadline` | The date and time of the deadline until which the project will raise funds. After the deadline, the project is determined to be "successful" or "failed" depending on whether it reached its funding goal. |
| `launched_at` | The date and time at which the project was made public and started raising funds. |
| `duration_days` | deadline - launched_at. The fundraising duration of the project in days. |
| `goal` | The funding goal of the project. |
| `name` | The title of the project. |
| `usd_pledged` | The amount pledged towards the project in USD. |
| `staff_pick` | True if the Kickstarter staff have chosen to feature the project in their "Projects We Love" program, false otherwise. 

# Data Cleaning and Preparation

```{r}
#to import dataset
kick_data <- read.csv("kick_data.csv", header=TRUE)
```

```{r}
head(kick_data)
```

```{r}
dim(kick_data)
```

```{r}
colnames(kick_data)
```

```{r}
#filtering out duplicate entries by keeping the most recent item and additionally filtering by the unique column number in X
kick_data.f <- kick_data %>% group_by(id)%>% filter(state_changed_at == max(state_changed_at)) %>% filter(X == max(X)) %>% rename(sub_id=X)
```


```{r}
#selecting columns to use, temporarily selecting id columns for merging and identification
to_use<- c("id", "sub_id",  "state", "backers_count", "blurb", "slug", "category", "converted_pledged_amount", "country","location", "deadline", "launched_at", "goal", "name","usd_pledged","staff_pick", "spotlight")
kick_data.f <- kick_data.f %>% select(to_use)

```


```{r}
#converting launched_at and deadline to datetime formats
dts <- kick_data.f$launched_at
mydates = dts
class(mydates) = c('POSIXt','POSIXct')
kick_data.f$launched_at <- as.Date(mydates)

dts <- kick_data.f$deadline
mydates = dts
class(mydates) = c('POSIXt','POSIXct')
kick_data.f$deadline <-  as.Date(mydates)
```


```{r}
#calcuation duration of project in days
kick_data.f$duration_days <- as.numeric(difftime(kick_data.f$deadline ,kick_data.f$launched_at , units = c("days")))
```

```{r}
#parsing our json data to extract categories 
ParseJSONColumn <- function(x)  {
     str_c("[ ", str_c(x, collapse = ",", sep=" "), " ]") %>% 
      fromJSON(flatten = T) %>% 
     as.tibble()
} 

JSONcolumn_data <- kick_data.f  %>% 
    select(id, category)  %>% 
    map_dfc(.f = ParseJSONColumn)  %>% select(value, name, slug) %>% rename(id = value, category = name, subcategory = slug)


kick_data.f <- kick_data.f %>% select(-category) %>% rename(description=slug)

kick_data.f <-  merge(kick_data.f, JSONcolumn_data ,by="id")

```

```{r}
#extracting locations through string splits
parsed_location <- kick_data.f %>% separate(location, c("foo", "bar"), '\"displayable_name\":\"') %>% select(foo,bar) %>% separate(bar, c("location", "foo2"), '\",\"') %>%  separate(location, c("foo", "location"), ', ')  %>% select(location)

kick_data.f$location <- parsed_location$location

northeast =c('CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA')
midwest = c('IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD')
west =c('AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AL', 'CA', 'HI', 'OR', 'WA')
south = c('MD', 'DE', 'VA', 'WV','KY','TN','NC', "SC", "FL", "GA", "AL", "MS", "LA", "AK", "TK", "OK")
kick_data.f$location <- ifelse(kick_data.f$location %in% northeast, "US-Northeast", ifelse(kick_data.f$location %in% midwest, "US-Midwest", ifelse(kick_data.f$location %in% west, "US-West", ifelse(kick_data.f$location %in% south, "US-South", kick_data.f$location))))
```


```{r}
#converting types of columns
kick_data.f$location <- as.factor(kick_data.f$location)
kick_data.f$blurb <- as.character(kick_data.f$blurb)
kick_data.f$description <- as.character(kick_data.f$description)
kick_data.f$name <- as.character(kick_data.f$name)
kick_data.f$category <- as.factor(kick_data.f$category)
```

```{r}
#replacing dashes with spaces in descriptions column and removing _truncated_ tag in blurb
kick_data.f$description <- gsub("-", " ",kick_data.f$description)
```

```{r}
#creating predictor column
kick_data.f$successful<- as.factor(as.numeric(ifelse(kick_data.f$state=="successful", 1, 0)))
```


```{r}
#filtering out the live projects, but storing them in a seperate place
kick_data.f.live <- kick_data.f %>% filter(state=="live")
kick_data.f <- kick_data.f %>% filter(state != "live")
```

```{r}
#double checking that we all desired columns and removing ids
to_use<- c("state", "successful", "backers_count", "blurb", "category", "description", "converted_pledged_amount", "country","location", "deadline", "launched_at", "duration_days", "goal", "name","usd_pledged","staff_pick", "spotlight")


kick_data.f <- kick_data.f %>% select(to_use)
```

```{r}
#remove converted_pledged_amount because it is the same as usd_pledged, just converted differently 
kick_data.f <- subset(kick_data.f, select = -converted_pledged_amount)
```


```{r}
#final state
str(kick_data.f)
dim(kick_data.f)
```

```{r, include=FALSE}
#check which variables have NAs 
sapply(kick_data.f, function(x) any(is.na(x))) # any(is.na(var)) is very useful, it returns T/F for each var
apply(kick_data.f, 2, function(x) any(is.na(x))) == TRUE  # apply function by columns: column-wise missing 
apply(kick_data.f, 1, function(x) any(is.na(x)))  # apply function by rows: row-wise missing
```

The only columns with NAs appear to be `location` and `blurb.` There is one NA in `blurb` and 117 NAs in `location.` Since 117 is a significant number of NAs and there is no reasonable way to interpolate the names of cities in which projects are based, `location` is not considered in the following models.  

```{r}
sum(is.na(kick_data.f$location))
sum(is.na(kick_data.f$blurb))
```

The original dataset includes 85523 observations and 16 variables. For computational reasons, a quarter of the dataset is randomly sampled and used for the models instead of the entire dataset. 

```{r}
set.seed(10)
#First randomly sample a quarter of the data for computational purposes
sample.size <- round(dim(kick_data.f) * 0.25)
index.sample <- sample(nrow(kick_data.f), sample.size)
data <- kick_data.f[index.sample, ] #dim(data) = 21381    16
```

The final dataset used for the models includes 21381 observations and 16 variables. 

The final dataset is split into training and testing sets, 70% for training and 30% for testing. 

```{r}
#split the data into training and testing sets 
set.seed(10)
# use the 70/30 rule: set the training dataset size to be 70% of data size and testing to be 30%
train.size <- round(dim(data) * 0.7)
index.t <- sample(nrow(data), train.size)
train <- data[index.t, ] #dim(train) = 14967    16
test <- data[-index.t, ] #dim(test) = 6414    16
```

#Words

```{r}
mycorpus1 <- VCorpus( VectorSource(kick_data.f$blurb))
mycorpus1
typeof(mycorpus1)   ## It is a list
# inspect the first corpus 
inspect(mycorpus1[[1]])
# or use `as.character` to extract the text
as.character(mycorpus1[[1]])
```

```{r}
mycorpus_clean <- tm_map(mycorpus1, content_transformer(tolower))
mycorpus_clean <- tm_map(mycorpus_clean, removeWords, stopwords("english"))
mycorpus_clean <- tm_map(mycorpus_clean, removePunctuation)
mycorpus_clean <- tm_map(mycorpus_clean, removeNumbers)
mycorpus_clean <- tm_map(mycorpus_clean, stemDocument, lazy = TRUE)   
lapply(mycorpus_clean[4:5], as.character)
```


```{r}
dtm1 <- DocumentTermMatrix( mycorpus_clean )   ## library = collection of words for all documents
dtm1
```


```{r}
threshold <- .005*length(mycorpus_clean)   # 1% of the total documents 
words.10 <- findFreqTerms(dtm1, lowfreq=threshold)  # words appearing at least among 1% of the documents
length(words.10)

dtm.10<- DocumentTermMatrix(mycorpus_clean, control = list(dictionary = words.10))  
dim(as.matrix(dtm.10))
colnames(dtm.10)[1:50]
```

```{r}
names(data)
# Combine the original data with the text matrix
data1.temp <- data.frame(kick_data.f,as.matrix(dtm.10) )   
dim(data1.temp)
names(data1.temp)[1:30]
str(data1.temp)
# data2 consists of date, rating and all the top 1% words
#ata2 <- data1.temp[, c(1,7, 8,11, 14:ncol(data1.temp))]
#names(data2)[1:20]
#dim(data2)
```

```{r}
data2
```


```{r}
y <- data1.temp$successful
X <- as.matrix(data1.temp[, -c(1,2)]) # we can use as.matrix directly here
set.seed(2)

#### Be careful to run the following LASSO.
result.lasso <- cv.glmnet(X, y, alpha=.99, family="binomial")  # 10 minutes in my MAC
#save(result.lasso, file="/Users/lzhao/Dropbox/STAT471/Data/TextMining.RData")

### or try `sparse.model.matrix()` which is much faster
#X1 <- sparse.model.matrix(successful~., data=data1.temp)[, -1]
#dim(X1)
#result.lasso.1 <- cv.glmnet(X1, y, alpha=.99, family="binomial")  # 1.5 minutes in my MAC
#plot(result.lasso.1)
plot(result.lasso)
```

# Model Overview

# Models

## Logistic Regression

### Elastic Net 

First, Elastic Net is used to limit the size of the coefficient vector and impose sparcity among the coefficients. 

```{r}
# Prepare the design matrix and response for LASSO
X <- model.matrix(successful ~ data$backers_count + data$category
                  + data$country + data$deadline + data$launched_at
                  + data$duration_days + data$goal + data$usd_pledged + data$staff_pick, data)[, -1]
Y <- data$successful
dim(X)
length(Y)
```

```{r}
set.seed(10) # to have same sets of K folds
fit1.cv <- cv.glmnet(X, Y, alpha=.9, family="binomial", nfolds = 10, type.measure = "deviance")  
plot(fit1.cv)
```

fit1.cv\$lambda.1se is chosen because it has fewer variables.

```{r include=FALSE}
coef.1se <- coef(fit1.cv, s="lambda.1se")  
coef.1se <- coef.1se[which(coef.1se !=0),] #pull out coefficients that aren't 0 
coef.1se   
```

Replacing the variables for which factor levels (`categoryPunk`, `countryMX`) are outputted by the above Elastic Net with the corresponding categorical variable, the following eight variables are outputted by Elastic Net: `category`, `country`, `duration_days`, `staff_pick`, `backers_count`, `deadline`, `goal`, and `usd_pledged`. 

### Logistic Regression

To locate a set of important features, a logistic regression using `glm()` is run with the variables obtained from Elastic Net above on the training dataset.

```{r include = FALSE}
fit.logit.1 <- glm(successful ~ category + country + duration_days + staff_pick + backers_count
                   + deadline + goal + usd_pledged, family=binomial, data=train)
summary(fit.logit.1)
```
```{r include=FALSE}
Anova(fit.logit.1)
```

### Backward Elimination 

Backward elimination is performed to only keep variables that are significant at the 0.01 level. 

First, `staff_pick` is removed because it has the highest p-value. 

```{r, include=FALSE}
# Remove staff_pick because it has highest the p-value
fit.logit.2 <- update(fit.logit.1, .~. -staff_pick)
Anova(fit.logit.2)
```

After backward elimination, the following seven variables are significant in the logistic regression model: `category`, `country`, `duration_days`, `backers_count`, `deadline`, `goal`, `usd_pledged`.

### Evaluate the Logistic Regression Model 

To evaluate the logistic regression model, `fit.logit.2`, the 1/2 thresholding rule is used for predicting `successful` on the testing data.

```{r include=FALSE}
#fit.logit.2
#Predict successful on the testing data.
fit.logit.2.test <- predict(fit.logit.2, test, type="response")
fit.logit.2.pred <- ifelse(fit.logit.2.test >= 1/2, "1", "0")
mce.logit <- mean(fit.logit.2.pred != test$successful)
mce.logit
```

The testing missclassification error of the logistic regression model is about 0.097. Not bad! 

## Random Forest

# Comparison of Methods 

# Final Model 

# Validity of Results and Future Improvement

# Conclusion

# Appendix

# Works Cited

