---
title: "[CATCHY TITLE]"
author:
- Hoyt Gong
- Younghu Park
- Hiyori Yoshida
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(car, dplyr, ggplot2, glmnet, tidyverse, jsonlite, lubridate, zoo,tm, SnowballC, RColorBrewer, wordcloud)


```

# Summary 

# Description of the Problem

First, some terms are defined to help explain how Kickstarter works. 

A **creator** is the person or team behind a project idea. 

A **project** is a finite work with a clear goal for which a creator is seeking funding.

The **funding goal** is the amount of money that a creator seeks to raise for their project. 

**Backers** are people who **pledge** money to help creators make their projects a reality. 

Creators who seek funding on Kickstarter want their projects to succeed so they can bring their ideas to life. Funding on Kickstarter is all-or-nothing. This means the creator will not receive the money people have pledged toward their project unless the project reaches its funding goal. Thus, "success" in this case is reaching the funding goal. As demonstrated in Kickstarter's [Creator Handbook](https://www.kickstarter.com/help/handbook?ref=whatarebasics), creators put a lot of effort into designing their project, funding, promotion, and communication to increase their chances of reaching their funding goal. 

The goal of this paper is to use attributes of existing Kickstarter projects in classifiers to predict whether a project will succeed or fail. Determining the atrributes of a project that are most valuable in predicting success can help creators design their projects to maximize their chances of reaching their funding goal. 

# Description of the Data

[Web Robots](https://webrobots.io/kickstarter-datasets/) has been scraping data from Kickstarter projects monthly starting March 2016. The most recent data from November 2019 is used in this project. The response variable that is predicted in this project is `successful`, a categorical variable that is either 1 for "successful" or 0 for "failed". The original data includes "canceled" as a state, which are converted to "failed". This is because it is assumed that canceled projects would likely have failed. The following independent variables are considered in the analysis: 

|Independent Variable|Description|
|-----------------|--------------------------------|
| `backers_count` | The number of backers that have pledged some amount of money to the project. |
| `blurb` | A short summary at the top of project pages that describe/promote the project. |
| `category` | The category of the project. This dataset includes 160 different categories, including "public Art," "software," "narrative film," etc. |
| `slug` -> `description`| Select keywords to describe the project, renamed to description |
| `country` | The country in which the project is based. | 
| `location` | The city and state in wich the project is based. |
| `deadline` | The date and time of the deadline until which the project will raise funds. After the deadline, the project is determined to be "successful" or "failed" depending on whether it reached its funding goal. |
| `launched_at` | The date and time at which the project was made public and started raising funds. |
| `duration_days` | deadline - launched_at. The fundraising duration of the project in days. |
| `goal` | The funding goal of the project. |
| `name` | The title of the project. |
| `usd_pledged` | The amount pledged towards the project in USD. |
| `staff_pick` | True if the Kickstarter staff have chosen to feature the project in their "Projects We Love" program, false otherwise. 

# Data Cleaning and Preparation

```{r}
#to import dataset
kick_data <- read.csv("kick_data.csv", header=TRUE)
```

```{r include = FALSE}
head(kick_data)
```

```{r}
dim(kick_data)
```

```{r}
colnames(kick_data)
```

First, because our data includes multiple entries for each company whenever the state of their project changes (i.e. from live to failed), we needed to get the rows with the companies most recent states. We did this by finding the max value of `state_changed_at`. Additionally, some rows seemed to contain duplicates of companies, with only the `url` to be different, so we simply utilized the autogenerated index row `X` to drop the duplicates and renamed this as `sub_id` for identification purposes.

```{r}
#filtering out duplicate entries by keeping the most recent item and additionally filtering by the unique column number in X
kick_data.f <- kick_data %>% group_by(id)%>% filter(state_changed_at == max(state_changed_at)) %>% filter(X == max(X)) %>% rename(sub_id=X)
```

Next, we wanted to filter down our columns to the ones we detailed above. We also included the `id` and `sub_id` columns to uniquely identify each row. 

```{r}
#selecting columns to use, temporarily selecting id columns for merging and identification
to_use<- c("id", "sub_id",  "state", "backers_count", "blurb", "slug", "category", "converted_pledged_amount", "country","location", "deadline", "launched_at", "goal", "name","usd_pledged","staff_pick")
kick_data.f <- kick_data.f %>% select(to_use)
```


Next, we converted the `launched_at` and `deadline` columns to date formats.

```{r}
#converting launched_at and deadline to datetime formats
dts <- kick_data.f$launched_at
mydates = dts
class(mydates) = c('POSIXt','POSIXct')
kick_data.f$launched_at <- as.Date(mydates)

dts <- kick_data.f$deadline
mydates = dts
class(mydates) = c('POSIXt','POSIXct')
kick_data.f$deadline <-  as.Date(mydates)
```

Then, we proceeded to create our `duration_days` column to indicate the timespan of the project.

```{r}
#calcuation duration of project in days
kick_data.f$duration_days <- as.numeric(difftime(kick_data.f$deadline ,kick_data.f$launched_at , units = c("days")))
```


Next, we needed to handle to json data within the `category` and `location` columns. For the `category` column, we were able to successfully transform the data into a json and extract the `category` and `subcategory` columns. However, the location column had some erroneous formatting, and we thus manually extracted the name with string manipulation searching for the `displayable_name` tag. In order to cut down the number of unique locations, we also only preserved the country of origin. For projects in the United States, we seperated them by US Region.

```{r}
#parsing our json data to extract categories 
ParseJSONColumn <- function(x)  {
     str_c("[ ", str_c(x, collapse = ",", sep=" "), " ]") %>% 
      fromJSON(flatten = T) %>% 
     as.tibble()
} 

JSONcolumn_data <- kick_data.f  %>% 
    select(id, category)  %>% 
    map_dfc(.f = ParseJSONColumn)  %>% select(value, name, slug) %>% rename(id = value, category = name, subcategory = slug)


kick_data.f <- kick_data.f %>% select(-category) %>% rename(description=slug)

kick_data.f <-  merge(kick_data.f, JSONcolumn_data ,by="id")

#extracting locations through string splits
parsed_location <- kick_data.f %>% separate(location, c("foo", "bar"), '\"displayable_name\":\"') %>% select(foo,bar) %>% separate(bar, c("location", "foo2"), '\",\"') %>%  separate(location, c("foo", "location"), ', ')  %>% select(location)

kick_data.f$location <- parsed_location$location

northeast =c('CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA')
midwest = c('IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD')
west =c('AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AL', 'CA', 'HI', 'OR', 'WA')
south = c('MD', 'DE', 'VA', 'WV','KY','TN','NC', "SC", "FL", "GA", "AL", "MS", "LA", "AK", "TK", "OK")
kick_data.f$location <- ifelse(kick_data.f$location %in% northeast, "US-Northeast", ifelse(kick_data.f$location %in% midwest, "US-Midwest", ifelse(kick_data.f$location %in% west, "US-West", ifelse(kick_data.f$location %in% south, "US-South", kick_data.f$location))))

```


Next, we converted various columns to our desired variable type. We also further cleaned the `description` column to get rid of the hyphens between each words.
```{r}
#converting types of columns
kick_data.f$location <- as.factor(kick_data.f$location)
kick_data.f$blurb <- as.character(kick_data.f$blurb)
kick_data.f$description <- as.character(kick_data.f$description)
kick_data.f$name <- as.character(kick_data.f$name)
kick_data.f$category <- as.factor(kick_data.f$category)

#replacing dashes with spaces in descriptions column and removing _truncated_ tag in blurb
kick_data.f$description <- gsub("-", " ",kick_data.f$description)
```


Finally, we created our indicator column to predict on. First, we filtered out the `live` projects as those were still ongoing and thus were not properly concluded. For the remaining projects, we classified all projects with the `state`, `successful` as 1, while `failed` and `canceled` projects would be 0.

```{r}
#filtering out the live projects, but storing them in a seperate place
kick_data.f.live <- kick_data.f %>% filter(state=="live")
kick_data.f <- kick_data.f %>% filter(state != "live")
```


```{r}
#creating predictor column
kick_data.f$successful<- as.factor(as.numeric(ifelse(kick_data.f$state=="successful", 1, 0)))
```

As only last sanity check, we took only the columns we wanted to move forward with.
```{r}
#double checking that we all desired columns and removing ids
to_use<- c("state", "successful", "backers_count", "blurb", "category", "description", "converted_pledged_amount", "country","location", "deadline", "launched_at", "duration_days", "goal", "name","usd_pledged","staff_pick")

kick_data.f <- kick_data.f %>% select(to_use)
```

```{r}
#remove converted_pledged_amount because it is the same as usd_pledged, just converted differently 
kick_data.f <- subset(kick_data.f, select = -converted_pledged_amount)
```

```{r, include=FALSE}
#check which variables have NAs 
sapply(kick_data.f, function(x) any(is.na(x))) # any(is.na(var)) is very useful, it returns T/F for each var
apply(kick_data.f, 2, function(x) any(is.na(x))) == TRUE  # apply function by columns: column-wise missing 
apply(kick_data.f, 1, function(x) any(is.na(x)))  # apply function by rows: row-wise missing
```

The only columns with NAs appear to be `location` and `blurb.` There is one NA in `blurb` and 117 NAs in `location.` Since there is no reasonable way to interpolate the names of cities in which projects are based or the blurb of the project, rows that have NA values in `location` or `blurb` are removed from the dataset.   

```{r include =FALSE}
sum(is.na(kick_data.f$location))
sum(is.na(kick_data.f$blurb))
```

```{r include=FALSE}
kick_data.f <- na.omit(kick_data.f)
dim(kick_data.f)
str(kick_data.f)
```

The dataset includes 85405 observations and 15 variables. For computational reasons, a quarter of the dataset is randomly sampled and used for the models instead of the entire dataset. 

```{r}
set.seed(10)
#First randomly sample a quarter of the data for computational purposes
sample.size <- round(dim(kick_data.f) * 0.25)
index.sample <- sample(nrow(kick_data.f), sample.size)
data <- kick_data.f[index.sample, ] #dim(data) = 21351        15
```

The final dataset used for the models includes 21351 observations and 15 variables. 

The final dataset is split into training and testing sets, 70% for training and 30% for testing. 

```{r}
#split the data into training and testing sets 
set.seed(10)

# use the 70/30 rule: set the training dataset size to be 70% of data size and testing to be 30%
train.size <- round(dim(data) * 0.7)
index.t <- sample(nrow(data), train.size)
train <- data[index.t, ] #dim(train) = 14946    15
test <- data[-index.t, ] #dim(test) = 6405    15
```

# Exploratory Data Analysis

As shown below, 22 countries are represented in the dataset. 71% of the projects are based in the US. 
```{r}
#Bar chart of Countries 
counts <- table(data$country)
barplot(counts, main="Country Distribution",
   xlab="Country", las=2, col=brewer.pal(22, name="YlGn"))
```

```{r include=FALSE}
15163/sum(counts)
```

```{r include = FALSE}
#category EDA 
cat_counts <- table(data$category)
names(cat_counts)[which.max(cat_counts)]
max(cat_counts)

names(cat_counts)[which.min(cat_counts)]
min(cat_counts)
```

Among the 160 categories included in the dataset, "Web" is the most popular, with 429 projects. ???show top 10???

???Backers Count Histogram???

The distribution of `backers_count` is highly skewed right. The mean is 139 backers, with a large standard deviation of 980 backers. The median is 26 backers, with a large  interquartile range of 86-3=83 backers. Since the minimum number of backers is 0 while the maximum is 91585, `backers_count` has a wide range. Because this high 

```{r}
data.t1 <- data %>% filter(backers_count < 500)


data.t1 %>% count()
```


```{r}
boxplot.backers <- boxplot(data.t1$backers_count, breaks = 40, xlab = "Backers Count", 
               main = "Histogram of Backers Count",col = brewer.pal(22, name="YlGn"))
#remove everything above 1950

#histogram of backers_count
histogram.backers <- hist(data.t1$backers_count, breaks = 40, xlab = "Backers Count", 
               main = "Histogram of Backers Count",col = brewer.pal(22, name="YlGn"))
summary(data$backers_count)
sd(data$backers_count)
```

???goal???
Like `backers_count`, the distribution of `goal` is skewed right. The mean fundraising goal is about \$42900 with a large standard deviation of \$978141. The median is about $5000, with a large IQR of 15000-1500=\$13500. The large range of \$100000000 also shows that `goal` differs greatly among projects. 

```{r}
#histogram of goal
histogram <- hist(data$goal, breaks = 40, xlab = "Goal ($)", ylab = "Frequency", 
               main = "Histogram of Goal in Dollars",col = brewer.pal(2, name="YlGn"))
summary(data$goal)
sd(data$goal)
```

???usd_pledged???

The histogram of `usd_pledged`, the amount of money pledged to a project in USD, also looks very similar to `backers_count` and `goal`. The mean is \$12975, with a large standard deviation of \$96928. The median is $1500 with a large IQR of 6381-97=\$6284. The range is also large, \$6225355.  

```{r}
#histogram of usd_pledged
histogram <- hist(data$usd_pledged, breaks = 40, xlab = "Money Pledged ($)", ylab = "Frequency", 
               main = "Histogram of Money Pledged in Dollars",col = brewer.pal(2, name="YlGn"))
summary(data$usd_pledged)
sd(data$usd_pledged)
```

As the histogram below shows, the distribution of deadlines for projects is skewed left, which is expected since the projects are those that exist in November 2019. The majority of projects have a deadline in the range 2014-2019. Late 2014 to early 2015 appears to have the most deadlines for projects. 

```{r}
#histogram of deadlines 
histogram <- hist(data$deadline, breaks = 40, xlab = "Deadline Date", ylab = "Frequency", 
               main = "Histogram of Deadline",col = brewer.pal(30, name="YlGn"))
```

The histogram of `duration_days` shows two modes, one at around 30 days and the other at around 60 days. This suggests that most projects are live for about a month, while others are live for about 2 months. The mean is about 32.7 days, with a standard deviation of 12 days. 

```{r}
#histogram of duration_days
histogram <- hist(data$duration_days, breaks = 40, xlab = "Duration in Days", ylab = "Frequency", 
               main = "Histogram of Duration of Project in Days",col = brewer.pal(30, name="YlGn"))
summary(data$duration_days)
sd(data$duration_days)
```

The bar chart of `staff_pick` shows that most projects are not chosen to be featured by the staff as part of the "Projects We Love" program. This suggests that the projects that are chosen are particularly featureable.

```{r}
#Bar chart of staff_pick 
counts <- table(data$staff_pick)
barplot(counts, main="Staff Pick",
   xlab="Staff Pick", las=2, col=brewer.pal(2, name="YlGn"))
```

# Model Overview

# Models

## Logistic Regression

### Elastic Net 

First, Elastic Net is used to limit the size of the coefficient vector and impose sparcity among the coefficients. `location` is excluded because `country` is a more general variable that describes the location in which the project is based, and including both would lead to high colinearity. 

```{r}
# Prepare the design matrix and response for LASSO
X <- model.matrix(successful ~ data$backers_count + data$category
                  + data$country + data$deadline + data$launched_at
                  + data$duration_days + data$goal + data$usd_pledged + data$staff_pick, data)[, -1]
Y <- data$successful
dim(X)
length(Y)
```

```{r}
set.seed(10) # to have same sets of K folds
fit1.cv <- cv.glmnet(X, Y, alpha=.9, family="binomial", nfolds = 10, type.measure = "deviance")  
plot(fit1.cv)
```

fit1.cv\$lambda.1se is chosen because both fit1.cv\$lambda.min and fit1.cv\$lambda.1se return the same number of variables.

```{r include=FALSE}
coef.1se <- coef(fit1.cv, s="lambda.1se")  
coef.1se <- coef.1se[which(coef.1se !=0),] #pull out coefficients that aren't 0 
coef.1se   
```

Replacing the variables for which factor levels (`categoryPunk`, `countryMX`) are outputted by the above Elastic Net with the corresponding categorical variable, the following eight variables are outputted by Elastic Net: `category`, `backers_count`, `country`, `deadline`, `duration_days`, `staff_pick`, `goal`, and `usd_pledged`. 

### Logistic Regression

To locate a set of important features, a logistic regression using `glm()` is run with the variables obtained from Elastic Net above on the training dataset.

```{r include = FALSE}
fit.logit.1 <- glm(successful ~ category + country + duration_days + staff_pick + backers_count
                   + deadline + goal + usd_pledged, family=binomial, data=train)
summary(fit.logit.1)
```
```{r include=FALSE}
Anova(fit.logit.1)
```

Since all variables are significant at the 0.01 level, the final logistic model without the text variables include the following eight variables: `category`, `backers_count`, `country`, `deadline`, `duration_days`, `staff_pick`, `goal`, and `usd_pledged`.

### Evaluate the Logistic Regression Model 

To evaluate the logistic regression model, `fit.logit.1`, the 1/2 thresholding rule is used for predicting `successful` on the testing data.

```{r include=FALSE}
#fit.logit.2
#Predict successful on the testing data.
fit.logit.test <- predict(fit.logit.1, test, type="response")
fit.logit.pred <- ifelse(fit.logit.test >= 1/2, "1", "0")
mce.logit <- mean(fit.logit.pred != test$successful)
mce.logit
```

The testing missclassification error of the logistic regression model is about 0.087. Not bad! 

## Random Forest


## Text Mining 

As an alternative model, we wanted to explore whether or not we could utilize text data contained within the `blurb` column to predict whether or not a company will be successful.

First, we isolated the text data:
```{r}
data1.text <- data$blurb
print(data1.text[1:5]) # view a few documents
```

Overall, it appears as if the blurbs are roughly the same length. This is most likely due to the fact that there is a restriction on length for these blurbs on Kickstarter.

### Corpus: a collection of text

Here, we move onto creating our corpus.

```{r}
mycorpus1 <- VCorpus( VectorSource(data1.text))
mycorpus1
```

### Data cleaning using `tm_map()`

Before we transform our text into a word frequency matrix, we needed to standardized the text. Similar to our model in class, we used  `removeNumbers()`, `removePunctuation()`, `removeWords()`, `stemDocument()`, and `stripWhitespace()` to go about cleaning our data.

```{r}
mycorpus_clean <- tm_map(mycorpus1, content_transformer(tolower))
mycorpus_clean <- tm_map(mycorpus_clean, removeWords, stopwords("english"))
mycorpus_clean <- tm_map(mycorpus_clean, removePunctuation)
mycorpus_clean <- tm_map(mycorpus_clean, removeNumbers)
mycorpus_clean <- tm_map(mycorpus_clean, stemDocument, lazy = TRUE)
```

### Word frequency matrix

Now we transform each review into a word frequency matrix using the function `DocumentTermMatrix()`.

```{r}
dtm1 <- DocumentTermMatrix( mycorpus_clean )   ## library = collection of words for all documents
```


Intially, there are a total of 26541 words that could be utilized as predictors. This is clearly, too many, so we decided to cut down the bag to only include the words appearing at least 1% (or the frequency of your choice) of the time to reduce the dimension of the features extracted to be analyzed. 

```{r}
threshold <- .01*length(mycorpus_clean)   # 1% of the total documents 
words.10 <- findFreqTerms(dtm1, lowfreq=threshold)  # words appearing at least among 1% of the documents
```

```{r}
dtm.10<- DocumentTermMatrix(mycorpus_clean, control = list(dictionary = words.10))  
dim(as.matrix(dtm.10))
colnames(dtm.10)[1:25]
```

In the end, we ended up with 167 possible words to utilize as predictors.

### Data preparation
Next, we used lasso to see what words are actually meaningful predictors for our data.

```{r, eval=F}
#data.text <- data %>% select(successful, category, country, duration_days, backers_count, deadline, goal, usd_pledged)
# Combine the original data with the text matrix
data1.temp <- data.frame(as.matrix(dtm.10))   
data1.temp$successful <- data$successful
data2 <- data1.temp %>% select(successful, everything())
```

```{r}
#splitting our data into train test splits
set.seed(1)
n=nrow(data2)
test.index <- sample(n, 0.3*n)
length(test.index)
data2.test <- data2[test.index, ] # only keep rating and the texts
data2.train <- data2[-test.index, ]
names(data2.train)
```

###LASSO

```{r}
y.n <- data2.train$successful
X.n <- as.matrix(data2.train[, -c(1)]) # we can use as.matrix directly here
set.seed(1)
#### Be careful to run the following LASSO.
result.lasso.text <- cv.glmnet(X.n, y.n, alpha=0, family="binomial")  # 10 minutes in my MAC
# save(result.lasso, file="/Users/lzhao/Dropbox/STAT471/Data/TextMining.RData")
```

```{r}
plot(result.lasso.text)
beta.lasso <- coef(result.lasso.text, s="lambda.min")   # output lasso estimates
beta <- beta.lasso[which(beta.lasso !=0),] # non zero beta's
beta <- as.matrix(beta)
beta <- rownames(beta)
```

Taking a look at our lasso plot, we observe that lasso actually did not cut down any of the words. This suggests that there are no absolutely significant words that are included the description that can lead to success versus failure. Nevertheless, we decided to run a logistic regression on the word frequencies.

```{r}
# Input the words from LASSO into glm
glm.input <- as.formula(paste("successful", "~", paste(beta[-1],collapse = "+"))) # prepare the formulae
result.glm <- glm(glm.input, family=binomial, data2.train ) # instantly 

# Testing errors

predict.glm <- predict(result.glm, data2.test, type = "response")
class.glm <- rep("0", nrow(data2.test))
class.glm[predict.glm > .5] <- "1"

testerror.glm <- mean(data2.test$successful != class.glm)
testerror.glm
#pROC::roc(data2.test$successful, predict.glm, plot=T) 
```

Using just the word frequency data, we got a much higher testing error of 0.36455. Using ROC, we saw an area under the curve of 0.6801. This higher error and lower accuracy shows that text data may not be the best in terms of helping us predict whether or not the product will be successful. This shows the overall subjectivity of the ratings.


###Additional Supplement: Wordclouds

Here, we wanted to see whether or not we could visually see a difference between the words common to successful products versus words common to the ones that failed.

```{r, message=FALSE, warning=FALSE, results= TRUE}
result.glm.coef <- coef(result.glm)

# pick up the positive coef's which are positively related to the prob of being a good review
success.glm <- result.glm.coef[which(result.glm.coef > 0)]
success.glm <-success.glm[-1]  # took intercept out

cor.special <- brewer.pal(8,"Dark2")  # set up a pretty color scheme
success.fre <- sort(success.glm, decreasing = TRUE) # sort the coef's
#round(success.fre, 4)[1:20] # leading 20 positive words, amazing!

# hist(as.matrix(good.fre), breaks=30, col="red") 
success.word <- names(success.fre)  # good words with a decreasing order in the coeff's

fail.glm <- result.glm.coef[which(result.glm.coef < 0)]
# names(bad.glm)[1:50]
fail.fre <- sort(-fail.glm, decreasing = TRUE)
#round(fail.fre, 4)[1:20]
# hist(as.matrix(bad.fre), breaks=30, col="green")
fail.word <- names(fail.fre)

par(mfrow=c(1,2))
cor.special <- brewer.pal(8,"Dark2") 
wordcloud(good.word[1:20], good.fre[1:20], 
          colors=cor.special, ordered.colors=F)
wordcloud(bad.word[1:20], bad.fre[1:20], 
          color=cor.special, ordered.colors=F)

```

Here, we see that "pin", which indicates whether or not it was on the front page, was the highest scoring word. This word was most likely autogenerated by Kickstarter and placed into the blurb for the projects that were doing well. Further analysis would look to probably remove this word in order to not give the model an advantage. Interestingly, we see words related to art/culture such as "comic" "modern" enamel" and "illust" to be dominant in successful businesses, whereas words related to business such as "app", "busi", "even", "product" were more common to the projects that failed. This seems to suggest that companies with more creative/art focused projects do better on kickstarter as opposed to "strictly business" companies.

# Comparison of Methods 

# Final Model 

# Validity of Results and Future Improvement

# Conclusion

# Appendix

# Works Cited

